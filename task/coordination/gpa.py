"""
GPA Gateway - Interface to the General-Purpose Agent.

Execution Flow:
1. Filter message history to include only GPA-relevant messages (restore tool call state)
2. Augment last user message with coordinator instructions
3. Call GPA via AsyncDial client with streaming enabled
4. Process streaming chunks: content, attachments, and stage propagation
5. Mirror GPA stages to coordinator stages for UI visibility
6. Persist GPA state (tool calls) in choice for conversation continuity

External I/O: HTTP calls to GPA endpoint via aidial_client
Side Effects: Opens/closes stages on choice, sets state and custom_content
"""

from copy import deepcopy
from typing import Optional, Any

from aidial_client import AsyncDial
from aidial_sdk.chat_completion import Role, Choice, Request, Message, CustomContent, Stage, Attachment
from pydantic import StrictStr

from task.stage_util import StageProcessor

# State keys for identifying and restoring GPA conversation context
_IS_GPA = "is_gpa"
_GPA_MESSAGES = "gpa_messages"


class GPAGateway:
    """
    Gateway for communicating with the General-Purpose Agent.
    Handles stage propagation, attachment forwarding, and tool call state restoration.
    """

    def __init__(self, endpoint: str):
        """
        Initialize gateway with DIAL endpoint.
        
        Args:
            endpoint: DIAL Core endpoint URL (e.g., http://localhost:8080)
        """
        self.endpoint = endpoint

    async def response(
            self,
            choice: Choice,
            stage: Stage,
            request: Request,
            additional_instructions: Optional[str]
    ) -> Message:
        """
        Send user request to GPA and stream the response with full DIAL feature support.
        
        Key Features:
        - Stage propagation: GPA's internal stages are mirrored to coordinator
        - Attachment forwarding: Images/files from GPA appear in coordinator response
        - State persistence: Tool call history preserved for multi-turn conversations
        
        Args:
            choice: DIAL choice for state storage and custom content
            stage: Parent stage to append content
            request: Original request with message history
            additional_instructions: Optional context from coordinator
            
        Returns:
            Assistant message with GPA's response content
        """
        # Initialize AsyncDial client - compatible with OpenAI API
        client = AsyncDial(base_url=self.endpoint, api_version='2025-01-01-preview')

        # Prepare messages filtered to GPA-relevant history
        messages = self.__prepare_gpa_messages(request, additional_instructions)

        # Call GPA with streaming; x-conversation-id needed for RAG context
        stream = await client.chat.completions.create(
            stream=True,
            messages=messages,
            model="general-purpose-agent",
            extra_headers={'x-conversation-id': request.headers.get('x-conversation-id', '')}
        )

        # Accumulators for streaming response
        content = ""
        result_custom_content = CustomContent(attachments=[])
        stages_map: dict[int, Stage] = {}  # Track mirrored stages by index

        # Process streaming chunks
        async for chunk in stream:
            if not chunk.choices:
                continue

            delta = chunk.choices[0].delta
            print(f"[GPA Delta] {delta}")  # Debug: observe incoming data structure

            # Accumulate content and stream to stage
            if delta.content:
                content += delta.content
                stage.append_content(delta.content)

            # Handle custom_content: attachments, state, and stage propagation
            if delta.custom_content:
                # Forward attachments (images, files generated by GPA)
                if delta.custom_content.attachments:
                    for att in delta.custom_content.attachments:
                        result_custom_content.attachments.append(att)

                # Preserve GPA state (tool call history for conversation continuity)
                if delta.custom_content.state:
                    result_custom_content.state = delta.custom_content.state

                # Stage propagation magic: mirror GPA's stages to coordinator
                cc_dict = delta.custom_content.dict(exclude_none=True)
                if 'stages' in cc_dict:
                    for stg in cc_dict['stages']:
                        idx = stg.get('index')
                        if idx is None:
                            continue

                        if idx in stages_map:
                            # Update existing mirrored stage
                            mirrored_stage = stages_map[idx]
                            if 'content' in stg:
                                mirrored_stage.append_content(stg['content'])
                            if 'attachments' in stg:
                                for att_dict in stg['attachments']:
                                    mirrored_stage.add_attachment(**att_dict)
                            if stg.get('status') == 'completed':
                                StageProcessor.close_stage_safely(mirrored_stage)
                        else:
                            # Open new mirrored stage with GPA's stage name
                            stage_name = stg.get('name', f'GPA Stage {idx}')
                            new_stage = StageProcessor.open_stage(choice, stage_name)
                            stages_map[idx] = new_stage
                            # Apply initial content if present
                            if 'content' in stg:
                                new_stage.append_content(stg['content'])

        # Propagate custom_content to choice
        # Note: aidial_client and aidial_sdk use different Pydantic models for Attachment
        if result_custom_content.attachments:
            for att in result_custom_content.attachments:
                # Convert between SDK versions using dict serialization
                choice.add_attachment(**att.dict(exclude_none=True))

        # Save GPA state for future requests - enables multi-turn tool call conversations
        gpa_state = {_IS_GPA: True, _GPA_MESSAGES: result_custom_content.state}
        choice.set_state(gpa_state)

        return Message(role=Role.ASSISTANT, content=content)

    def __prepare_gpa_messages(self, request: Request, additional_instructions: Optional[str]) -> list[dict[str, Any]]:
        """
        Prepare message history filtered to GPA-relevant context.
        
        GPA needs its tool call history restored for proper conversation continuity.
        We identify GPA messages by the _IS_GPA flag in state and restore the
        original state format (_GPA_MESSAGES) that GPA expects.
        
        Args:
            request: Original request with full message history
            additional_instructions: Optional context to augment last message
            
        Returns:
            List of message dicts ready for GPA API call
        """
        res_messages: list[dict[str, Any]] = []

        # Filter history to include only GPA-related exchanges
        messages = request.messages
        for idx in range(len(messages)):
            msg = messages[idx]

            # Check if this assistant message was from a GPA interaction
            if msg.role == Role.ASSISTANT and msg.custom_content:
                state = msg.custom_content.state
                if state and state.get(_IS_GPA):
                    # Include the preceding user message
                    if idx > 0:
                        res_messages.append(messages[idx - 1].dict(exclude_none=True))

                    # Restore GPA's expected state format from our wrapped format
                    msg_copy = deepcopy(msg)
                    msg_copy.custom_content.state = state.get(_GPA_MESSAGES)
                    res_messages.append(msg_copy.dict(exclude_none=True))

        # Add the current user message (always the last one)
        last_message = messages[-1].dict(exclude_none=True)
        
        # Augment with coordinator instructions if provided
        if additional_instructions:
            current_content = last_message.get('content', '')
            last_message['content'] = f"{current_content}\n\nAdditional context: {additional_instructions}"
        
        res_messages.append(last_message)

        return res_messages

